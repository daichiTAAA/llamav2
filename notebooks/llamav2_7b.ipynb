{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamav2を試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "[Databricksレポジトリ](https://github.com/databricks/databricks-ml-examples/tree/master/llm-models/llamav2)  \n",
    "[Qiita: MetaのLlama 2をDatabricksで動かしてみる](https://qiita.com/taka_yayoi/items/bc1da42144826da56ab4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Load model to text generation pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\")\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロンプトの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template, the format below is from: http://fastml.com/how-to-train-your-own-chatgpt-alpaca-style-part-one/\n",
    "\n",
    "# Prompt templates as follows could guide the model to follow instructions and respond to the input, and empirically it turns out to make Falcon models produce better responses\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to generate text\n",
    "def gen_text(prompts, use_template=False, **kwargs):\n",
    "    if use_template:\n",
    "        full_prompts = [\n",
    "            PROMPT_FOR_GENERATION_FORMAT.format(instruction=prompt)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        full_prompts = prompts\n",
    "\n",
    "    if \"batch_size\" not in kwargs:\n",
    "        kwargs[\"batch_size\"] = 1\n",
    "    \n",
    "    # the default max length is pretty small (20), which would cut the generated output in the middle, so it's necessary to increase the threshold to the complete response\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 2024\n",
    "\n",
    "    # configure other text generation arguments, see common configurable args here: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,  # Hugging Face sets pad_token_id to eos_token_id by default; setting here to not see redundant message\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    outputs = pipeline(full_prompts, **kwargs)\n",
    "    outputs = [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単一の入力で文書生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\transformers\\generation\\utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A large language model is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models are designed to learn the patterns and structures of language by exposure to a wide range of texts, and can be used for a variety of applications such as language translation, text summarization, and language generation.\n",
      "\n",
      "Some examples of large language models include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers): Developed by Google in 2018, BERT is a powerful language model that has achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks. BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence.\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach): Developed in 2019, RoBERTa is a variant of BERT that was specifically designed for text classification tasks. RoBERTa uses a modified version of the BERT architecture and training procedure to improve its performance on long-tail and out-of-vocabulary words.\n",
      "3. DistilBERT (Distilled BERT): Developed in 2019, DistilBERT is a smaller and more efficient variant of BERT that has been trained to match the performance of the full BERT model while requiring fewer computational resources. DistilBERT uses a distillation technique to compress the knowledge of the full BERT model into a smaller model that is more efficient to train and deploy.\n",
      "4. Longformer (Long-range dependence transformer): Developed in 2020, Longformer is a language model that is specifically designed to handle long-range dependencies in text. Longformer uses a novel attention mechanism that allows it to process input sequences of arbitrary length, making it well-suited for tasks such as machine translation and text summarization.\n",
      "\n",
      "These are just a few examples of the many large language models that have been developed in recent years. Each of these models has achieved state-of-the-art results on a wide range of NLP tasks, and they have all contributed to the rapid advancement of the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"What is a large language model?\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日本語は？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "大規模言語モデル（Big Language Model、BLM）とは、自然言語processing（NLP）における一種の人工知能（AI）で、大規模なデータセットを用いて、自然言語の表現を処理することができるように設計されている。\n",
      "\n",
      "BLMは、通常、大量の文書やデータセットを用いて、言語モデルを学習することができる。これらのデータセットには、単語や文、文章などの自然言語の表現が含まれている。BLMは、これらのデータセットを用いて、自然言語の表現を処理するための特殊な neural network を学習する。\n",
      "\n",
      "BLMは、以下のような役割を果たすことができる。\n",
      "\n",
      "1. 文本生成：BLMを使用すると、新しい文本を生成することができます。これには、単語や文、文章などの自然言語の表現が含まれています。\n",
      "2. 文本理解：BLMを使用すると、既存の文本を理解することができます。これには、単語や文、文章などの自然言語の表現が含まれています。\n",
      "3. 単語の定義：BLMを使用すると、単語の定義を学習することができます。これには、単語の意味や、単語が使用される文脈などが含まれています。\n",
      "4. 文法分析：BLMを使\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"大規模言語モデルとは？\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "桃太郎は、日本の民話の登場人物であり、「桃太郎」という名前を持つ。彼は、桃の木の下で生まれ、桃太郎と名付けられた。彼は、妖精の姉妹たちとともに暮らし、桃の木の下で暮らしていた。\n",
      "\n",
      "ある日、桃太郎は、妖精たちとともに桃の木の下で遊んでいた。彼らは、桃の木の下で遊んでいるうちに、桃太郎が妖精たちに対して、「桃太郎の物語」を語り始めた。\n",
      "\n",
      "彼らは、桃太郎が妖精たちに語った物語を、「桃太郎の物語」と呼んだ。彼らは、桃太郎の物語を聞いて、彼の勇敢さと優しさに感動した。\n",
      "\n",
      "桃太郎の物語は、妖精たちにとっては、大切な物語であり、彼らの心に深く刻まれた。彼らは、桃太郎の物語を、彼の勇敢さと優しさを讃えるために、彼の名前を呼び続けた。\n",
      "\n",
      "以上が、桃太郎の物語の要点です。彼の物語は、勇敢さと優しさを讃えるために、妖精たちにとっては、大切な物語であり、彼らの心に深く刻まれた。\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"\"\"桃太郎の物語を教えてください。                   \n",
    "\"\"\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 桃太郎の物語を知らないようだった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 桃太郎の物語でfine-tuningを行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# その前にDollyデータセットでfine-tuningを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes (0.41.1)\n",
      "Available versions: 0.41.1, 0.41.0, 0.40.2, 0.40.1.post1, 0.40.0.post4, 0.40.0, 0.39.1, 0.39.0, 0.38.1, 0.37.2, 0.35.4, 0.35.0\n",
      "  INSTALLED: 0.41.1\n",
      "  LATEST:    0.41.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip index is currently an experimental command. It may be removed/changed in a future release without prior warning.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip index versions bitsandbytes --index-url=https://jllllll.github.io/bitsandbytes-windows-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\n",
      "Collecting bitsandbytes==0.39.1\n",
      "  Downloading https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl (137.9 MB)\n",
      "     ---------------------------------------- 0.0/137.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/137.9 MB 1.7 MB/s eta 0:01:24\n",
      "     ---------------------------------------- 0.6/137.9 MB 7.4 MB/s eta 0:00:19\n",
      "     --------------------------------------- 1.6/137.9 MB 11.3 MB/s eta 0:00:13\n",
      "      -------------------------------------- 3.2/137.9 MB 16.9 MB/s eta 0:00:08\n",
      "     - ------------------------------------- 5.3/137.9 MB 22.6 MB/s eta 0:00:06\n",
      "     -- ------------------------------------ 7.5/137.9 MB 26.8 MB/s eta 0:00:05\n",
      "     -- ------------------------------------ 9.7/137.9 MB 29.5 MB/s eta 0:00:05\n",
      "     --- ---------------------------------- 11.9/137.9 MB 43.5 MB/s eta 0:00:03\n",
      "     --- ---------------------------------- 14.0/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     ---- --------------------------------- 16.3/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     ----- -------------------------------- 18.5/137.9 MB 46.9 MB/s eta 0:00:03\n",
      "     ----- -------------------------------- 20.8/137.9 MB 46.9 MB/s eta 0:00:03\n",
      "     ------ ------------------------------- 23.0/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     ------ ------------------------------- 25.1/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     ------- ------------------------------ 27.3/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     -------- ----------------------------- 29.3/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     -------- ----------------------------- 31.5/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 33.5/137.9 MB 43.5 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 35.7/137.9 MB 43.7 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 37.5/137.9 MB 43.7 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 39.7/137.9 MB 43.7 MB/s eta 0:00:03\n",
      "     ----------- -------------------------- 41.8/137.9 MB 43.7 MB/s eta 0:00:03\n",
      "     ------------ ------------------------- 44.0/137.9 MB 46.7 MB/s eta 0:00:03\n",
      "     ------------ ------------------------- 46.2/137.9 MB 43.5 MB/s eta 0:00:03\n",
      "     ------------- ------------------------ 48.1/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     ------------- ------------------------ 49.5/137.9 MB 43.5 MB/s eta 0:00:03\n",
      "     -------------- ----------------------- 51.5/137.9 MB 40.9 MB/s eta 0:00:03\n",
      "     -------------- ----------------------- 53.5/137.9 MB 40.9 MB/s eta 0:00:03\n",
      "     --------------- ---------------------- 55.1/137.9 MB 38.6 MB/s eta 0:00:03\n",
      "     --------------- ---------------------- 56.8/137.9 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------- --------------------- 58.9/137.9 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------- --------------------- 61.1/137.9 MB 40.9 MB/s eta 0:00:02\n",
      "     ----------------- -------------------- 63.3/137.9 MB 40.9 MB/s eta 0:00:02\n",
      "     ------------------ ------------------- 65.4/137.9 MB 43.5 MB/s eta 0:00:02\n",
      "     ------------------ ------------------- 67.6/137.9 MB 46.9 MB/s eta 0:00:02\n",
      "     ------------------- ------------------ 69.8/137.9 MB 46.9 MB/s eta 0:00:02\n",
      "     ------------------- ------------------ 72.0/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     -------------------- ----------------- 74.3/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     --------------------- ---------------- 76.5/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     --------------------- ---------------- 78.7/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 80.9/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 83.2/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     ----------------------- -------------- 85.4/137.9 MB 46.9 MB/s eta 0:00:02\n",
      "     ------------------------ ------------- 87.6/137.9 MB 46.9 MB/s eta 0:00:02\n",
      "     ------------------------ ------------- 89.8/137.9 MB 46.7 MB/s eta 0:00:02\n",
      "     ------------------------- ------------ 91.9/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 94.2/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 96.3/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 98.4/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     -------------------------- ---------- 100.7/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     --------------------------- --------- 102.9/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 105.1/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 107.3/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 109.5/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 111.7/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 113.9/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 116.2/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 118.3/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 120.6/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 122.8/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 125.0/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 127.1/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 129.2/137.9 MB 46.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 131.2/137.9 MB 43.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 133.4/137.9 MB 43.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  135.5/137.9 MB 43.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  137.8/137.9 MB 43.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  137.9/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  137.9/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  137.9/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  137.9/137.9 MB 46.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- 137.9/137.9 MB 21.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages (from bitsandbytes==0.39.1) (1.11.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages (from scipy->bitsandbytes==0.39.1) (1.25.1)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.39.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install bitsandbytes==0.39.1 --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77aab366f7a40858111be177cbac7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/noda7/miniconda3/envs/llamav2/bin')}\n",
      "  warn(msg)\n",
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: C:\\Users\\noda7\\miniconda3\\envs\\llamav2 did not contain ['cudart64_110.dll', 'cudart64_120.dll', 'cudart64_12.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8adb6b85634fd4b1c00379a6d3b0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40e4d8723bf4c72a8728fea8d1b7f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\noda7\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\peft\\utils\\other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab38f344cd1e4634b903058c1a1b532f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0d33c0c1cf47179c54247058fbd57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1256, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.0987, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 1.0643, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 1.0855, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.0849, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 1.0905, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 1.0833, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 1.077, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
      "{'loss': 1.0695, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 1.0476, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
      "{'train_runtime': 2364.0086, 'train_samples_per_second': 6.768, 'train_steps_per_second': 0.423, 'train_loss': 1.082691375732422, 'epoch': 1.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6d17759a56406792ea5907c2f5c12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b58aa3f39ff4440ab74d897e5f569c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)85b5b0bbb9/README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310c7d089dc44f5ebc2761802967c57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b0bbb9/USE_POLICY.md:   0%|          | 0.00/4.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e36ae4ef4864d7e9353f719bd407438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49995ed549e84f598edf8e544f5bad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b5b0bbb9/LICENSE.txt:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee1e07870eb45ccb096f2c67db43b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a84cd92db884a84893b22f097dec3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5360a805164a698b0eea8b1252a4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0bbb9/.gitattributes:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83996e3365f456a9abe497e34a5bc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b5b0bbb9/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635ccbb3e004dc18f3b0e32bc4603a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfac684ea2064b3f81c591f48c3ce923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330401b890714096a5cd19a98ea4abb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e22e2e88674886a5becbb62945a17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nsible-Use-Guide.pdf:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aea528b7a87434bb331d1274defce35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a13df0418a84a3fa86ada26969ebcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/06 01:14:36 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf403301b779458db6c991bc13670937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2023/09/06 01:15:34 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - loralib (current: uninstalled, required: loralib)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf9804aec6f4399af728fb1c5e8b071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nif one get corona and you are self isolating and it is not severe, is there any meds that one can take?\\n\\n### Response: \\nYes, you can take paracetamol for fever and pain.\\n\\n### End:\\nHope this helps.\\n\\n### End of response\\n\\n### End of input\\n\\n### End of response\\n\\n### End of input\\n\\n### End of response\\n\\n### End of input\\n\\n### End of response\\n\\n### End of input\\n\\n### End of response\\n\\n### End of input\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Fine tune llama-2-7b-hf with QLORA\n",
    "# MAGIC\n",
    "# MAGIC [Llama 2](https://huggingface.co/meta-llama) is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. It is trained with 2T tokens and supports context length window upto 4K tokens. [Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) is the 7B pretrained model, converted for the Hugging Face Transformers format.\n",
    "# MAGIC\n",
    "# MAGIC This is to fine-tune [llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) models on the [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset.\n",
    "# MAGIC\n",
    "# MAGIC Environment for this notebook:\n",
    "# MAGIC - Runtime: 13.2 GPU ML Runtime\n",
    "# MAGIC - Instance: `g5.8xlarge` on AWS, `Standard_NV36ads_A10_v5` on Azure\n",
    "# MAGIC\n",
    "# MAGIC We leverage the PEFT library from Hugging Face, as well as QLoRA for more memory efficient finetuning.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to Huggingface to get access to the model\n",
    "notebook_login()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Install required packages\n",
    "# MAGIC\n",
    "# MAGIC Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %pip install git+https://github.com/huggingface/peft.git\n",
    "# MAGIC %pip install datasets==2.12.0 bitsandbytes==0.40.1 einops==0.6.1 trl==0.4.7\n",
    "# MAGIC %pip install torch==2.0.1 accelerate==0.21.0 transformers==4.31.0\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Dataset\n",
    "# MAGIC\n",
    "# MAGIC We will use the [databricks-dolly-15k ](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "  intro=INTRO_BLURB,\n",
    "  instruction_key=INSTRUCTION_KEY,\n",
    "  instruction=\"{instruction}\",\n",
    "  response_key=RESPONSE_KEY,\n",
    "  response=\"{response}\",\n",
    "  end_key=END_KEY\n",
    ")\n",
    "\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{input}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\"\"\".format(\n",
    "  intro=INTRO_BLURB,\n",
    "  instruction_key=INSTRUCTION_KEY,\n",
    "  instruction=\"{instruction}\",\n",
    "  input_key=INPUT_KEY,\n",
    "  input=\"{input}\",\n",
    "  response_key=RESPONSE_KEY,\n",
    "  response=\"{response}\",\n",
    "  end_key=END_KEY\n",
    ")\n",
    "\n",
    "def apply_prompt_template(examples):\n",
    "  instruction = examples[\"instruction\"]\n",
    "  response = examples[\"response\"]\n",
    "  context = examples.get(\"context\")\n",
    "\n",
    "  if context:\n",
    "    full_prompt = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "  else:\n",
    "    full_prompt = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "  return { \"text\": full_prompt }\n",
    "\n",
    "dataset = dataset.map(apply_prompt_template)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dataset[\"text\"][0]\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Loading the model\n",
    "# MAGIC\n",
    "# MAGIC In this section we will load the [LLaMAV2](), quantize it in 4bit and attach LoRA adapters on it.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "revision = \"351b2c357c69b4779bde72c0e7f7da639443d904\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=bnb_config,\n",
    "    revision=revision,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Load the configuration file in order to create the LoRA model. \n",
    "# MAGIC\n",
    "# MAGIC According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'k_proj', 'v_proj'] # Choose all linear layers from the model\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Loading the trainer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"/local_disk0/results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 500\n",
    "logging_steps = 100\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Then finally pass everthing to the trainer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Train the model\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Now let's train the model! Simply call `trainer.train()`\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save the LORA model\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "trainer.save_model(\"/local_disk0/llamav2-7b-lora-fine-tune\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Log the fine tuned model to MLFlow\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = \"/local_disk0/llamav2-7b-lora-fine-tune\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "# Download the Llama-2-7b-hf model snapshot from huggingface\n",
    "snapshot_location = snapshot_download(repo_id=config.base_model_name_or_path)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "class LLAMAQLORA(mlflow.pyfunc.PythonModel):\n",
    "  def load_context(self, context):\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts['repository'])\n",
    "    self.tokenizer.pad_token = tokenizer.eos_token\n",
    "    config = PeftConfig.from_pretrained(context.artifacts['lora'])\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "      context.artifacts['repository'], \n",
    "      return_dict=True, \n",
    "      load_in_4bit=True, \n",
    "      device_map={\"\":0},\n",
    "      trust_remote_code=True,\n",
    "    )\n",
    "    self.model = PeftModel.from_pretrained(base_model, context.artifacts['lora'])\n",
    "  \n",
    "  def predict(self, context, model_input):\n",
    "    prompt = model_input[\"prompt\"][0]\n",
    "    temperature = model_input.get(\"temperature\", [1.0])[0]\n",
    "    max_tokens = model_input.get(\"max_tokens\", [100])[0]\n",
    "    batch = self.tokenizer(prompt, padding=True, truncation=True,return_tensors='pt').to('cuda')\n",
    "    with torch.cuda.amp.autocast():\n",
    "      output_tokens = self.model.generate(\n",
    "          input_ids = batch.input_ids, \n",
    "          max_new_tokens=max_tokens,\n",
    "          temperature=temperature,\n",
    "          top_p=0.7,\n",
    "          num_return_sequences=1,\n",
    "          do_sample=True,\n",
    "          pad_token_id=tokenizer.eos_token_id,\n",
    "          eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "    generated_text = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema([\n",
    "    ColSpec(DataType.string, \"prompt\"), \n",
    "    ColSpec(DataType.double, \"temperature\"), \n",
    "    ColSpec(DataType.long, \"max_tokens\")])\n",
    "output_schema = Schema([ColSpec(DataType.string)])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Define input example\n",
    "input_example=pd.DataFrame({\n",
    "            \"prompt\":[\"what is ML?\"], \n",
    "            \"temperature\": [0.5],\n",
    "            \"max_tokens\": [100]})\n",
    "\n",
    "with mlflow.start_run() as run:  \n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=LLAMAQLORA(),\n",
    "        artifacts={'repository' : snapshot_location, \"lora\": peft_model_id},\n",
    "        pip_requirements=[\"torch\", \"transformers\", \"accelerate\", \"einops\", \"loralib\", \"bitsandbytes\", \"peft\"],\n",
    "        input_example=pd.DataFrame({\"prompt\":[\"what is ML?\"], \"temperature\": [0.5],\"max_tokens\": [100]}),\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Run model inference with the model logged in MLFlow.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "if one get corona and you are self isolating and it is not severe, is there any meds that one can take?\n",
    "\n",
    "### Response: \"\"\"\n",
    "# Load model as a PyFuncModel.\n",
    "run_id = run.info.run_id\n",
    "logged_model = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "text_example=pd.DataFrame({\n",
    "            \"prompt\":[prompt], \n",
    "            \"temperature\": [0.5],\n",
    "            \"max_tokens\": [100]})\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "loaded_model.predict(text_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuningできた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tuning modelにプロンプトを投げてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93b3ec6bcf2b4474b445bbe4ca02bb2f\n"
     ]
    }
   ],
   "source": [
    "run_id = run.info.run_id\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/06 01:31:59 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - loralib (current: uninstalled, required: loralib)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c51fb06a3504fd4a4e64f9fcadb6333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nHow can I make a business with LLM?\\n\\n### Response: \\n1. Make a business that can generate a lot of data\\n2. Use LLM to generate insights from the data\\n\\n### End:\\nThis is just an idea.\\n\\n### End of response:\\nThis is just an idea.\\n\\n### End of the response:\\nThis is just an idea.\\n\\n### End of the response:\\nThis is just an idea.\\n\\n### End of the response:\\nThis is just an'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "How can I make a business with LLM?\n",
    "\n",
    "### Response: \"\"\"\n",
    "# Load model as a PyFuncModel.\n",
    "run_id = run.info.run_id # 今回は93b3ec6bcf2b4474b445bbe4ca02bb2f\n",
    "logged_model = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "text_example=pd.DataFrame({\n",
    "            \"prompt\":[prompt], \n",
    "            \"temperature\": [0.5],\n",
    "            \"max_tokens\": [100]})\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "loaded_model.predict(text_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_tokensを増やしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/06 01:33:45 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - loralib (current: uninstalled, required: loralib)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8904a90e0b42c19b684d2294a67458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nHow can I make a business with LLM?\\n\\n### Response: \\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make a business with LLM by using it to automate tasks and processes, such as customer service, data analysis, and legal research.\\n\\n### End:\\nYou can make'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "How can I make a business with LLM?\n",
    "\n",
    "### Response: \"\"\"\n",
    "# Load model as a PyFuncModel.\n",
    "run_id = run.info.run_id # 今回は93b3ec6bcf2b4474b445bbe4ca02bb2f\n",
    "logged_model = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "text_example=pd.DataFrame({\n",
    "            \"prompt\":[prompt], \n",
    "            \"temperature\": [0.5],\n",
    "            \"max_tokens\": [300]})\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "loaded_model.predict(text_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ回答が繰り返されるようになった？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論速度を向上させる\n",
    "[Medium: GPTQ or bitsandbytes: Which Quantization Method to Use for LLMs — Examples with Llama 2](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are developing/deploying LLMs on consumer hardware, I would recommend the following:\n",
    "* Fine-tune the LLM with bitsandbytes nf4 and QLoRa\n",
    "* Merge the adapter into the LLM\n",
    "* Quantize the resulting model with GPTQ 4-bit\n",
    "* Deploy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
