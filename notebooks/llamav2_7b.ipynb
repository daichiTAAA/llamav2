{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamav2を試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "[Databricksレポジトリ](https://github.com/databricks/databricks-ml-examples/tree/master/llm-models/llamav2)  \n",
    "[Qiita: MetaのLlama 2をDatabricksで動かしてみる](https://qiita.com/taka_yayoi/items/bc1da42144826da56ab4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Load model to text generation pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "revision = \"0ede8dd71e923db6258295621d817ca8714516d4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\")\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    revision=revision,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Required tokenizer setting for batch inference\n",
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロンプトの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template, the format below is from: http://fastml.com/how-to-train-your-own-chatgpt-alpaca-style-part-one/\n",
    "\n",
    "# Prompt templates as follows could guide the model to follow instructions and respond to the input, and empirically it turns out to make Falcon models produce better responses\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to generate text\n",
    "def gen_text(prompts, use_template=False, **kwargs):\n",
    "    if use_template:\n",
    "        full_prompts = [\n",
    "            PROMPT_FOR_GENERATION_FORMAT.format(instruction=prompt)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        full_prompts = prompts\n",
    "\n",
    "    if \"batch_size\" not in kwargs:\n",
    "        kwargs[\"batch_size\"] = 1\n",
    "    \n",
    "    # the default max length is pretty small (20), which would cut the generated output in the middle, so it's necessary to increase the threshold to the complete response\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 2024\n",
    "\n",
    "    # configure other text generation arguments, see common configurable args here: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,  # Hugging Face sets pad_token_id to eos_token_id by default; setting here to not see redundant message\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    outputs = pipeline(full_prompts, **kwargs)\n",
    "    outputs = [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単一の入力で文書生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noda7\\miniconda3\\envs\\llamav2\\lib\\site-packages\\transformers\\generation\\utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A large language model is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models are designed to learn the patterns and structures of language by exposure to a wide range of texts, and can be used for a variety of applications such as language translation, text summarization, and language generation.\n",
      "\n",
      "Some examples of large language models include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers): Developed by Google in 2018, BERT is a powerful language model that has achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks. BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence.\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach): Developed in 2019, RoBERTa is a variant of BERT that was specifically designed for text classification tasks. RoBERTa uses a modified version of the BERT architecture and training procedure to improve its performance on long-tail and out-of-vocabulary words.\n",
      "3. DistilBERT (Distilled BERT): Developed in 2019, DistilBERT is a smaller and more efficient variant of BERT that has been trained to match the performance of the full BERT model while requiring fewer computational resources. DistilBERT uses a distillation technique to compress the knowledge of the full BERT model into a smaller model that is more efficient to train and deploy.\n",
      "4. Longformer (Long-range dependence transformer): Developed in 2020, Longformer is a language model that is specifically designed to handle long-range dependencies in text. Longformer uses a novel attention mechanism that allows it to process input sequences of arbitrary length, making it well-suited for tasks such as machine translation and text summarization.\n",
      "\n",
      "These are just a few examples of the many large language models that have been developed in recent years. Each of these models has achieved state-of-the-art results on a wide range of NLP tasks, and they have all contributed to the rapid advancement of the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"What is a large language model?\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日本語は？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "大規模言語モデル（Big Language Model、BLM）とは、自然言語processing（NLP）における一種の人工知能（AI）で、大規模なデータセットを用いて、自然言語の表現を処理することができるように設計されている。\n",
      "\n",
      "BLMは、通常、大量の文書やデータセットを用いて、言語モデルを学習することができる。これらのデータセットには、単語や文、文章などの自然言語の表現が含まれている。BLMは、これらのデータセットを用いて、自然言語の表現を処理するための特殊な neural network を学習する。\n",
      "\n",
      "BLMは、以下のような役割を果たすことができる。\n",
      "\n",
      "1. 文本生成：BLMを使用すると、新しい文本を生成することができます。これには、単語や文、文章などの自然言語の表現が含まれています。\n",
      "2. 文本理解：BLMを使用すると、既存の文本を理解することができます。これには、単語や文、文章などの自然言語の表現が含まれています。\n",
      "3. 単語の定義：BLMを使用すると、単語の定義を学習することができます。これには、単語の意味や、単語が使用される文脈などが含まれています。\n",
      "4. 文法分析：BLMを使\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"大規模言語モデルとは？\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "桃太郎は、日本の民話の登場人物であり、「桃太郎」という名前を持つ。彼は、桃の木の下で生まれ、桃太郎と名付けられた。彼は、妖精の姉妹たちとともに暮らし、桃の木の下で暮らしていた。\n",
      "\n",
      "ある日、桃太郎は、妖精たちとともに桃の木の下で遊んでいた。彼らは、桃の木の下で遊んでいるうちに、桃太郎が妖精たちに対して、「桃太郎の物語」を語り始めた。\n",
      "\n",
      "彼らは、桃太郎が妖精たちに語った物語を、「桃太郎の物語」と呼んだ。彼らは、桃太郎の物語を聞いて、彼の勇敢さと優しさに感動した。\n",
      "\n",
      "桃太郎の物語は、妖精たちにとっては、大切な物語であり、彼らの心に深く刻まれた。彼らは、桃太郎の物語を、彼の勇敢さと優しさを讃えるために、彼の名前を呼び続けた。\n",
      "\n",
      "以上が、桃太郎の物語の要点です。彼の物語は、勇敢さと優しさを讃えるために、妖精たちにとっては、大切な物語であり、彼らの心に深く刻まれた。\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"\"\"桃太郎の物語を教えてください。                   \n",
    "\"\"\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 桃太郎の物語を知らないようだった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 桃太郎の物語でfine-tuningを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
